{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some text pre processing tasks\n",
    "#imports\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [............................................................................] 127831 / 127831"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../data/combined_data.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "url=\"https://drive.google.com/uc?id=13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P\"\n",
    "path=\"../data\"\n",
    "wget.download(url,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Good case Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>I have to jiggle the plug to get it to line up...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>If you have several dozen or several hundred c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>If you are Razr owner...you must have this!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Needless to say I wasted my money.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>What a waste of money and time!.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  sentiment\n",
       "0           0  So there is no way for me to plug it in here i...          0\n",
       "1           1                         Good case Excellent value.          1\n",
       "2           2                             Great for the jawbone.          1\n",
       "3           3  Tied to charger for conversations lasting more...          0\n",
       "4           4                                  The mic is great.          1\n",
       "5           5  I have to jiggle the plug to get it to line up...          0\n",
       "6           6  If you have several dozen or several hundred c...          0\n",
       "7           7        If you are Razr owner...you must have this!          1\n",
       "8           8                 Needless to say I wasted my money.          0\n",
       "9           9                   What a waste of money and time!.          0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the data\n",
    "\n",
    "def get_data(path):\n",
    "    data = pd.read_csv(path,header=0)\n",
    "    return data\n",
    "\n",
    "path=\"../data/combined_data.csv\"\n",
    "\n",
    "data = get_data(path)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>I think food should have flavor and texture an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>Overall I was not impressed and would not go b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>The whole experience was underwhelming and I t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>Then as if I hadn't wasted enough of my life t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1992 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  sentiment\n",
       "0     So there is no way for me to plug it in here i...          0\n",
       "1                            Good case Excellent value.          1\n",
       "2                                Great for the jawbone.          1\n",
       "3     Tied to charger for conversations lasting more...          0\n",
       "4                                     The mic is great.          1\n",
       "...                                                 ...        ...\n",
       "1987  I think food should have flavor and texture an...          0\n",
       "1988                           Appetite instantly gone.          0\n",
       "1989  Overall I was not impressed and would not go b...          0\n",
       "1990  The whole experience was underwhelming and I t...          0\n",
       "1991  Then as if I hadn't wasted enough of my life t...          0\n",
       "\n",
       "[1992 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop the unwanted column\n",
    "data.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'atalaia'...\n",
      "error: unable to create file Icon?: Invalid argument\n",
      "error: unable to create file atalaia/Icon?: Invalid argument\n",
      "error: unable to create file atalaia/assets/Icon?: Invalid argument\n",
      "error: unable to create file examples/Icon?: Invalid argument\n",
      "fatal: unable to checkout working tree\n",
      "warning: Clone succeeded, but checkout failed.\n",
      "You can inspect what was checked out with 'git status'\n",
      "and retry with 'git restore --source=HEAD :/'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clone package repository\n",
    "!git clone https://github.com/vallantin/atalaia.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from -r requirements.txt (line 1)) (3.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from -r requirements.txt (line 2)) (4.47.0)\n",
      "Requirement already satisfied: emoji in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from -r requirements.txt (line 3)) (0.6.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from -r requirements.txt (line 4)) (1.19.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from -r requirements.txt (line 5)) (1.0.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from -r requirements.txt (line 6)) (3.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from -r requirements.txt (line 7)) (0.23.1)\n",
      "Requirement already satisfied: gensim in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from -r requirements.txt (line 8)) (3.8.3)\n",
      "Requirement already satisfied: regex in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from nltk->-r requirements.txt (line 1)) (2020.7.14)\n",
      "Requirement already satisfied: click in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from nltk->-r requirements.txt (line 1)) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from nltk->-r requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from pandas->-r requirements.txt (line 5)) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from pandas->-r requirements.txt (line 5)) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 7)) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 7)) (1.4.1)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from gensim->-r requirements.txt (line 8)) (1.15.0)\n",
      "Requirement already satisfied: Cython==0.29.14 in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from gensim->-r requirements.txt (line 8)) (0.29.14)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from gensim->-r requirements.txt (line 8)) (2.1.0)\n",
      "Requirement already satisfied: requests in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from smart-open>=1.8.1->gensim->-r requirements.txt (line 8)) (2.24.0)\n",
      "Requirement already satisfied: boto3 in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from smart-open>=1.8.1->gensim->-r requirements.txt (line 8)) (1.14.41)\n",
      "Requirement already satisfied: boto in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from smart-open>=1.8.1->gensim->-r requirements.txt (line 8)) (2.49.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim->-r requirements.txt (line 8)) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim->-r requirements.txt (line 8)) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim->-r requirements.txt (line 8)) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim->-r requirements.txt (line 8)) (1.25.9)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.41 in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim->-r requirements.txt (line 8)) (1.17.41)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim->-r requirements.txt (line 8)) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim->-r requirements.txt (line 8)) (0.3.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages (from botocore<1.18.0,>=1.17.41->boto3->smart-open>=1.8.1->gensim->-r requirements.txt (line 8)) (0.15.2)\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing Atalaia.egg-info\\PKG-INFO\n",
      "writing dependency_links to Atalaia.egg-info\\dependency_links.txt\n",
      "writing top-level names to Atalaia.egg-info\\top_level.txt\n",
      "reading manifest file 'Atalaia.egg-info\\SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "writing manifest file 'Atalaia.egg-info\\SOURCES.txt'\n",
      "installing library code to build\\bdist.win-amd64\\egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build\\bdist.win-amd64\\egg\n",
      "creating build\\bdist.win-amd64\\egg\\atalaia\n",
      "creating build\\bdist.win-amd64\\egg\\atalaia\\assets\n",
      "copying build\\lib\\atalaia\\assets\\contractions.py -> build\\bdist.win-amd64\\egg\\atalaia\\assets\n",
      "copying build\\lib\\atalaia\\assets\\stopwords.py -> build\\bdist.win-amd64\\egg\\atalaia\\assets\n",
      "copying build\\lib\\atalaia\\assets\\__init__.py -> build\\bdist.win-amd64\\egg\\atalaia\\assets\n",
      "copying build\\lib\\atalaia\\atalaia.py -> build\\bdist.win-amd64\\egg\\atalaia\n",
      "copying build\\lib\\atalaia\\explore.py -> build\\bdist.win-amd64\\egg\\atalaia\n",
      "copying build\\lib\\atalaia\\files.py -> build\\bdist.win-amd64\\egg\\atalaia\n",
      "copying build\\lib\\atalaia\\strings.py -> build\\bdist.win-amd64\\egg\\atalaia\n",
      "copying build\\lib\\atalaia\\vectors.py -> build\\bdist.win-amd64\\egg\\atalaia\n",
      "copying build\\lib\\atalaia\\__init__.py -> build\\bdist.win-amd64\\egg\\atalaia\n",
      "byte-compiling build\\bdist.win-amd64\\egg\\atalaia\\assets\\contractions.py to contractions.cpython-37.pyc\n",
      "byte-compiling build\\bdist.win-amd64\\egg\\atalaia\\assets\\stopwords.py to stopwords.cpython-37.pyc\n",
      "byte-compiling build\\bdist.win-amd64\\egg\\atalaia\\assets\\__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build\\bdist.win-amd64\\egg\\atalaia\\atalaia.py to atalaia.cpython-37.pyc\n",
      "byte-compiling build\\bdist.win-amd64\\egg\\atalaia\\explore.py to explore.cpython-37.pyc\n",
      "byte-compiling build\\bdist.win-amd64\\egg\\atalaia\\files.py to files.cpython-37.pyc\n",
      "byte-compiling build\\bdist.win-amd64\\egg\\atalaia\\strings.py to strings.cpython-37.pyc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "zip_safe flag not set; analyzing archive contents...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "byte-compiling build\\bdist.win-amd64\\egg\\atalaia\\vectors.py to vectors.cpython-37.pyc\n",
      "byte-compiling build\\bdist.win-amd64\\egg\\atalaia\\__init__.py to __init__.cpython-37.pyc\n",
      "creating build\\bdist.win-amd64\\egg\\EGG-INFO\n",
      "copying Atalaia.egg-info\\PKG-INFO -> build\\bdist.win-amd64\\egg\\EGG-INFO\n",
      "copying Atalaia.egg-info\\SOURCES.txt -> build\\bdist.win-amd64\\egg\\EGG-INFO\n",
      "copying Atalaia.egg-info\\dependency_links.txt -> build\\bdist.win-amd64\\egg\\EGG-INFO\n",
      "copying Atalaia.egg-info\\top_level.txt -> build\\bdist.win-amd64\\egg\\EGG-INFO\n",
      "creating 'dist\\Atalaia-0.4.0-py3.7.egg' and adding 'build\\bdist.win-amd64\\egg' to it\n",
      "removing 'build\\bdist.win-amd64\\egg' (and everything under it)\n",
      "Processing Atalaia-0.4.0-py3.7.egg\n",
      "Removing c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages\\Atalaia-0.4.0-py3.7.egg\n",
      "Copying Atalaia-0.4.0-py3.7.egg to c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages\n",
      "Atalaia 0.4.0 is already the active version in easy-install.pth\n",
      "\n",
      "Installed c:\\users\\lakha\\anaconda3\\envs\\100days\\lib\\site-packages\\atalaia-0.4.0-py3.7.egg\n",
      "Processing dependencies for Atalaia==0.4.0\n",
      "Finished processing dependencies for Atalaia==0.4.0\n"
     ]
    }
   ],
   "source": [
    "# install packages requirements\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# install package\n",
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>no way plug us unless go converter</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>good case excellent value</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>great jawbone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>tied charger conversations lasting minutes maj...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>mic great</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  sentiment\n",
       "0           0                 no way plug us unless go converter          0\n",
       "1           1                          good case excellent value          1\n",
       "2           2                                      great jawbone          1\n",
       "3           3  tied charger conversations lasting minutes maj...          0\n",
       "4           4                                          mic great          1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from atalaia.atalaia import Atalaia\n",
    "\n",
    "#preprocess function from atalaia module\n",
    "def preprocess(panda_series):\n",
    "    atalaia = Atalaia('en')\n",
    "\n",
    "    # lower case everyting and remove double spaces\n",
    "    panda_series = (atalaia.lower_remove_white(t) for t in panda_series)\n",
    "\n",
    "    # expand contractions\n",
    "    panda_series = (atalaia.expand_contractions(t) for t in panda_series)\n",
    "\n",
    "    # remove punctuation\n",
    "    panda_series = (atalaia.remove_punctuation(t) for t in panda_series)\n",
    "\n",
    "    # remove numbers\n",
    "    panda_series = (atalaia.remove_numbers(t) for t in panda_series)\n",
    "\n",
    "    # remove stopwords\n",
    "    panda_series = (atalaia.remove_stopwords(t) for t in panda_series)\n",
    "\n",
    "    # remove excessive spaces\n",
    "    panda_series = (atalaia.remove_excessive_spaces(t) for t in panda_series)\n",
    "\n",
    "    return panda_series\n",
    "\n",
    "# preprocess it\n",
    "preprocessed_text = preprocess(data.text)\n",
    "\n",
    "# assign preprocessed texts to dataset\n",
    "data['text'] = list(preprocessed_text)\n",
    "\n",
    "# see data\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the dataset\n",
    "data = data.sample(frac=1)\n",
    "\n",
    "# separate all classes present on the dataset\n",
    "classes_dict = {}\n",
    "for label in [0,1]:\n",
    "    classes_dict[label] = data[data['sentiment'] == label]\n",
    "\n",
    "# get 80% of each label\n",
    "size = int(len(classes_dict[0].text) * 0.8)\n",
    "X_train = list(classes_dict[0].text[0:size])      + list(classes_dict[1].text[0:size])\n",
    "X_test  = list(classes_dict[0].text[size:])       + list(classes_dict[1].text[size:])\n",
    "y_train = list(classes_dict[0].sentiment[0:size]) + list(classes_dict[1].sentiment[0:size])\n",
    "y_test  = list(classes_dict[0].sentiment[size:])  + list(classes_dict[1].sentiment[size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to Numpy arrays\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[47, 41, 10, 319, 141]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's consider the vocab size as the number of words\n",
    "# that compose 90% of the vocabulary\n",
    "atalaia    = Atalaia('en')\n",
    "vocab_size = len(atalaia.representative_tokens(0.9, \n",
    "                                               ' '.join(X_train),\n",
    "                                               reverse=False))\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "# start tokenize\n",
    "tokenizer = Tokenizer(num_words=vocab_size, \n",
    "                      oov_token=oov_tok)\n",
    "\n",
    "# fit on training\n",
    "# we don't fit on test because, in real life, our model will have to deal with\n",
    "# words ir never saw before. So, it makes sense fitting only on training.\n",
    "# when it finds a word it never saw before, it will assign the \n",
    "# <OOV> tag to it.\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# get the word index\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# transform into sequences\n",
    "# this will assign a index to the tokens present on the corpus\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "# see the first sequence\n",
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 47,  41,  10, 319, 141,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define max_length \n",
    "max_length = 100\n",
    "\n",
    "# post: pad or truncate after sentence.\n",
    "# pre: pad or truncate before sentence.\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "\n",
    "padded = pad_sequences(sequences,\n",
    "                       maxlen=max_length, \n",
    "                       padding=padding_type, \n",
    "                       truncating=trunc_type)\n",
    "\n",
    "# tokenize and pad test sentences\n",
    "# thse will be used later on the model for accuracy test\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_test_padded    = pad_sequences(X_test_sequences,\n",
    "                                 maxlen=max_length, \n",
    "                                 padding=padding_type, \n",
    "                                 truncating=trunc_type)\n",
    "\n",
    "# check the first padded sentence. Notice that 0s were added to it\n",
    "# because it was shorter than 100\n",
    "padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded sentence:\n",
      "bland flavorless good way describing barely tepid meat ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n",
      "\n",
      "Original sentence\n",
      "bland flavorless good way describing barely tepid meat\n"
     ]
    }
   ],
   "source": [
    "# create the reverse word index\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "# create the decoder\n",
    "def text_decoder(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "# print the decoder output for one sentence and compare it to original\n",
    "print('Decoded sentence:')\n",
    "print(text_decoder(padded[1]))\n",
    "print('\\nOriginal sentence')\n",
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "100Days",
   "language": "python",
   "name": "100days"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
